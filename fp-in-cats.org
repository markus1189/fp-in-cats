* TODO Functional Programming with 'cats'
** TODO Introduction

** TODO Bad news?
<picture of cats crossed through, "NOT ABOUT CATS">

** TODO Worse news?
<complicated looking picture of math / categories>

** TODO Good news!

** TODO What is 'cats'
#+BEGIN_QUOTE
Lightweight, modular, and extensible library for functional programming.
#+END_QUOTE
- 0.6.0 currently released, 1.0 is not far (ETA: Q3 2016)
- Github: https://github.com/typelevel/cats
- Guide: http://typelevel.org/cats/
- Scaladoc: http://typelevel.org/cats/api/#package

- In the following we will have a very shallow look at how 'cats' can
  help you to write better code
** TODO Scala not just as a 'better Java'
- Scala allows higher level of abstraction
- less repetition and duplicated code, less maintenance, less bugs
- cats contains a lot of helpful constructs that can be used for this

** TODO How to get started
- at the beginning it _will_ be confusing to find stuff
- you will get a feeling for it ☺
- rough guide:

| package       | contains                     | examples          |
|---------------+------------------------------+-------------------|
| ~cats~        | type classes                 | Functor,Monoid    |
| ~cats.kernel~ | essential type classes       | Eq, Ordering      |
| ~cats.std~    | instances for standard Scala | List,Vector,Tuple |
| ~cats.data~   | data types                   | Xor,Validated     |
| ~cats.syntax~ | /optional/ syntactic sugar   |                   |

** TODO À la carte or the whole menu

- cats allows you to import stuff in many ways

#+BEGIN_SRC dot :file packages.png :cmdline -Tpng -Nfontsize=28
digraph {
rankdir=LR;
catsImplicits [label="cats.implicits"];

catsStdAll [label="cats.std.all"];
{ rank=same;
  catsStdFuture [label="cats.std.future"];
  catsStdOption [label="cats.std.option"];
  catsStdElse [label="cats.std.<...>"];
}

catsSyntaxAll [label="cats.syntax.all"];
{ rank=same;
  catsSyntaxTraverse [label="cats.syntax.traverse"];
  catsSyntaxSemigroup [label="cats.syntax.semigroup"];
  catsSyntaxElse [label="cats.syntax.<...>"];
}

catsImplicits -> catsStdAll;
catsImplicits -> catsSyntaxAll;

catsStdAll -> catsStdFuture;
catsStdAll -> catsStdOption;
catsStdAll -> catsStdElse;

catsSyntaxAll -> catsSyntaxTraverse;
catsSyntaxAll -> catsSyntaxSemigroup;
catsSyntaxAll -> catsSyntaxElse;
}
#+END_SRC

#+RESULTS:
[[file:packages.png]]

1) Import /everything/: ~cats.implicits._~
2) Import /packages/:   ~cats.<...>.all._~
3) Import /à la carte/: ~cats.std.future._~

** TODO Avoiding imports completely

- using chained package clauses, you can avoid imports completely

#+BEGIN_SRC scala
package de.my.app.concurrent

// becomes

package de.my
package app.concurrent
#+END_SRC

#+BEGIN_SRC scala
package de
package object my extends FutureInstances with ... 
#+END_SRC

** Where to find it
- typeclass? ~cats~, but also ~cats.kernel~ (since 0.6.0+), e.g. ~Foldable~, ~Eq~
- data type? ~cats.data~, e.g. ~Xor~, ~Validated~
- instances for typeclasses? ~cats.std.<...>~ or ~cats.kernel.<...>~

** TODO Typesafe equality
*** The sad story of '=='
#+BEGIN_SRC scala
type Email = String // TODO: make value class

def isInternal(email: Email): Boolean = {
  "admin@mail.com" == email ||
    "developer@mail.com" == email
}

isInternal("customer@mail.com") // => false
isInternal("admin@mail.com")    // => true
#+END_SRC

*** Refactoring time
#+BEGIN_SRC scala
case class Email(value: String) // extends AnyVal

def isInternal(email: Email): Boolean = {
  "admin@mail.com" == email ||
    "developer@mail.com" == email
}

isInternal(Email("customer@mail.com"))
isInternal(Email("admin@mail.com"))
#+END_SRC

*** An alternative: 'Eq'
#+BEGIN_SRC scala
import cats.kernel.Eq          // the Eq type class
import cats.syntax.eq._ // === syntax

case class Email(value: String) extends AnyVal
object Email {
  val eqEmail: Eq[Email] =
    Eq.fromUniversalEquals
}

def isInternal(email: Email): Boolean = {
  "admin@mail.com" === email ||
    "developer@mail.com" === email
}
#+END_SRC

*** Summary: ~Eq~
- equality based on type classes is safer
- allows you to catch errors during refactoring
-
- will *not* compile
- many ~Eq~ instances predefined
- still some work required for own types
** TODO Combining stuff (Monoid)
*** Apache Spark 1
- exercise: we want to calculate the following
  - number of words
  - word count _per word_
  - average word length
- and do this in *one traversal*
=> Monoids!
*** Apache Spark 2
#+BEGIN_SRC sbt
name := "Spark Sandbox"

scalaVersion := "2.10.6"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.6.1" % "provided",
  "org.typelevel" %% "cats-core" % "0.6.0")
#+END_SRC

#+BEGIN_SRC scala
import org.apache.spark._

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import cats._
import cats.kernel.Monoid
import cats.std.tuple._
import cats.kernel.std.option._
import cats.kernel.std.int._
import cats.syntax.semigroup._

// Value classes broken in 2.10: SI-7685
case class Min[A](value: A)  // extends AnyVal
case class Max[A](value: A)  // extends AnyVal
case class Prod[A](value: A) // extends AnyVal

object Min {
  implicit def minSemi[A: Order] = new Semigroup[Min[A]] {
    private val o = implicitly[Order[A]]
    def combine(x: Min[A], y: Min[A]): Min[A] = (x, y) match {
      case (Min(xx), Min(yy)) => Min(o.min(xx, yy))
    }
  }
}

object Max {
  implicit def maxSemi[A: Order] = new Semigroup[Max[A]] {
    private val o = implicitly[Order[A]]
    def combine(x: Max[A], y: Max[A]): Max[A] = (x, y) match {
      case (Max(xx), Max(yy)) => Max(o.max(xx, yy))
    }
  }
}

object Main {
  def main(args: Array[String]) = {
    def expand(word: String) = {
      (Option(Max(word.length)), Option(Min(word.length)), word.length, 1)
    }

    val file = "/home/markus/repos/clones/stack/README.md" // Should be some file on your system
    val conf = new SparkConf().setJars(Seq("/home/markus/src/scala/spark-sandbox/target/scala-2.10/Spark-Sandbox-assembly-0.1-SNAPSHOT.jar")).setMaster("spark://nixos:7077").setAppName("spark-cats")
    val sc = new SparkContext(conf)
    val data = sc.textFile(file).flatMap(_.split("""\s+""")).map(expand)
    val z = Monoid.empty[(Option[Max[Int]],Option[Min[Int]],Int,Int)]
    val (max,min,chars,words) = data.fold(z)(_ |+| _)
    println(s"""Finished calculation:
               |  - max word length: $max
               |  - min word length: $min
               |  - total characters: $chars
               |  - total words: $words
               |  - average word length: ${chars/words}
               |""".stripMargin)
  }
}
#+END_SRC

#+BEGIN_SRC sh
./spark-submit --class Main --master spark://nixos:7077 ~/src/scala/spark-sandbox/target/scala-2.10/Spark-Sandbox-assembly-0.1-SNAPSHOT.jar
#+END_SRC

#+RESULTS:

** TODO Validation
** TODO cats vs scalaz
- cats: still some things missing: ISet, IList, IMap
- cats: circe for JSON
- scalaz: no Task and Scalaz-Stream (soon: fs2)
- scalaz: monocle (lenses)
** TODO Pitfalls
- execution context necessary for Future
- IntelliJ will sometimes refuse to use syntax
